{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa7c2959-296e-4f05-86a0-6ae5656f1423",
   "metadata": {},
   "source": [
    "# BCVSPIN masterclass 2024 Hands-on session\n",
    "## Jets in particle physics\n",
    "A proton-proton collision can result in production of high energy quarks and/or gluons. In addition, other particles like a Higgs boson, W boson, etc can also decay into quarks. However, these quarks and gluons are not directly observed in the final state of the collision. This is because these quarks and gluons themselves tend to emit quarks and gluons at small angles or with small energy. This results in a series of collimated quarks and gluons. Once low energy scale is reached (of the order of 1 GeV), due to the nature of strong force [<sup>1</sup>](#confinement), these quarks and gluon combine to form hadrons. So, the end product of quark/gluon production in collisions are a bunch of collimated hadrons which are called **jets**. This means jets are proxies to the quarks and gluons produced in collisions, i.e. studying jets can give information about the particle that initiated the jet. \n",
    "\n",
    "\n",
    "\n",
    "But how do you decide which hadrons belong to one jet vs the other (for example in decay of W boson into two quarks)? There has to be a quantitative definition of jets which experimentalists and theorists can both agree on. So, what a jet is depends on a **jet definition**. A typical jet definition is a combination of three things:\n",
    "\n",
    "- A **jet algorithm**: it is a recipe on how to cluster hadrons to form jets. \n",
    "- A **jet radius**: it is a parameter of the jet algorithm which basically defines a distance above which two particles are not considered part of the same jet. \n",
    "- A **recombination scheme**: it is a scheme which defines how the kinematic properties of the jet are obtained from its consituents. For example, in the \"E-scheme\" the jet momenta and energy are obtained by a simple sum of components of consituents energy-momentum four vector. \n",
    "    \n",
    "There are a number of jet algorithms available. At the LHC the most widely used jet algorithm is anti-kt algorithm which is a type of sequential recombination algorithm [<sup>2</sup>](#generalized-kt). The sequential recombination algorithms try to invert the QCD process of quark/gluon branchings by iteratively recombining two particles into one until a few objects are left and then calls them jets. In case of anti-kt algorithm, the particles are clustered around the hadron with the largest transverse momentum.\n",
    "\n",
    "\n",
    "<div style=\"padding: 10px;\">\n",
    "    <center>\n",
    "        <img src=\"anti_kt.png\" alt=\"Anti kt algorithm clustering graph\">\n",
    "    </center>\n",
    "    <p>\n",
    "        This image shows simplified anti-kt algorithm clustering graph. Source: <a href=\"https://gsalam.web.cern.ch/repository/course-slides/2024-Oxford/Oxford-jets-lecture.pdf\">Gavin Salam jets lectures, slide 119 </a>. The x-axis shows the \"rapidity\" of particle (essentially a proxy for polar angle of the particle) and y-axis shows the transverse momentum of the particle in GeV. Since anti-kt algorithm clusters around particle with high transverse momentum, the starting point of clustering is particle near rapidity of 2 with the highest tranverse momentum. The first particle to cluster into it is the particle nearby it at around rapidity of 1.6. This is shown by the small semi-rectangle above the two particles. The clustering combines both the particles into one. The next particle to cluster into the resulting particle is the one closest to it, i.e. the particle at rapidity of around 1.1. This is shown by semi-rectangle above the particle at rapidity of 1.1 and particle at rapidity of around 1.8. The clustering and combining of particles goes on until all particles are clustered into a single particle (denoted by the black vertical line). Note that this is a very simplified picture. A more realistic picture would additionally need to consider the azimuthal angle of particles, and the jet radius according to the recipe in [<sup>2</sup>](#generalized-kt)\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "## Large-R jet tagging\n",
    "The jet radius $R$ is one of the most important parameters of jet algorithms. If its value is large, the resulting jet is called large-R jet. How large is large? This depends... ATLAS experiment considers jets with R=1 as large-R jets whereas CMS experiment uses R=0.8. Both experiments use R=0.4 for their small radius jets. Note that the $R$-parameter doesn't have to be fixed. One can have algorithms where $R$-parameter varies with the transverse momentum of the jet (the so-called Variable Radius jets). But why care about large-R jets? Let's take an example of Higgs boson decay to two bottom quarks. There are two regimes one can have:\n",
    "- Low Higgs transverse momentum (so called \"resolved\"): In this case the two quarks coming from Higgs decay have larger angular separation which results in the decay products being able to be reconstructed as two separate small-radius jets.\n",
    "- High Higgs transverse momentum (so called \"boosted\"): In this case the angular separation between the two quarks is small. So, the decay products could be clustered into two small-radius jets or one large-radius jet. However, in the boosted regime, we use large-radius jets. This is because the internal sub-structure of the large-radius jets can be exploited for identification of the jets. A rule-of-thumb for large-radius jets is that it makes sense to use them when the transverse momentum of the parent-particle is greater than twice the mass of the particle, i.e. $p_t > 2 \\cdot m$. In case of Higgs boson this would be $p_t \\sim 250$ GeV.\n",
    "\n",
    "<div style=\"display: flex\">\n",
    "<div style=\"flex: 1; padding: 10px;\">\n",
    "    <center>\n",
    "        <img src=\"resolved_regime.png\" alt=\"Resolved jet regime\" width=\"300\" height=\"200\">\n",
    "    </center>\n",
    "    <p>\n",
    "        This image shows resolved jets regime. Source: Gavin Salam, jets lecture\n",
    "    </p>\n",
    "</div>\n",
    "<div style=\"flex: 1; padding: 10px;\">\n",
    "    <center>\n",
    "        <img src=\"boosted_regime.png\" alt=\"Boosted jet regime\" width=\"300\" height=\"300\">\n",
    "    </center>\n",
    "    <p>\n",
    "        This image shows the boosted jet regime. Source: Gavin Salam, jets lecture\n",
    "    </p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<img src=\"JetTypes.png\" alt=\"Jet types\" width=\"400\" height=\"300\">\n",
    "\n",
    "Tagging, in context of jets in particle physics, refers to identifying objects that initiated the jet. For example, one can try to identify whether a small radius jet was initiated by a bottom quark or gluons or light quarks like up and down quarks (called \"b-tagging\"). One can also try to identify whether a large radius jet was initiated by a top quark, a W boson, or quark/gluons, etc. Large-R jet tagging is important both in search for new physics and for studying standard model processes. For example, Higgs boson can decay into a pair of bottom quarks. When Higgs momentum is large, both bottom quark jets will be reconstructed as one large-R jet. A X to bb tagger would be able to identify Higgs decaying to two bottom quarks. \n",
    "\n",
    "<hr>\n",
    "\n",
    "<span id=\"confinement\"> <sup>1 </sup> Strong force has a property called confinement which means that isolated objects with color can not be seen - what are observed are the colorless hadrons. Unlike electromagnetic force and gravitational force, the strong force does not drop with distance. If one tries to separate a quark from a hadron, the energy required to do so in order for it to be in observable scale would be much higher than pair-production energy for production of quark-antiquark pairs; so one would observe a bunch of hadrons instead.  </span>\n",
    "\n",
    "<span id=\"generalized-kt\"> <sup>2</sup> This part is for people who want more details. Anti-kt type of jet algorithms cluster particles as follows:\n",
    "- Start with a list of all particles in the event. (the \"particles\" don't have to be physical)\n",
    "- For two particles $i$ and $j$, compute inter-particle distance: $$d_{ij} = \\min (p_{T, i}^{2n}, p_{T,j}^{2n}) \\Delta R_{ij}^2$$ where $p_{T, i}$ is the transverse momentum of particle $i$, and $\\Delta R_{ij}$ is the Euclidean distance between the two particles in $\\eta-\\phi$ plane.\n",
    "- Compute another distance $d_{i, B} = p_{T, i}^{2n} R^2$ where $R$ is the radius parameter. \n",
    "- Compute $d_{ij}$ and $d_{i,B}$ for all objects in the initial list and find the minimum among all of them.\n",
    "- If $d_{i,B}$ is minimum, declare $i$ as jet, remove it from list, and repeat the step of computing distance again.\n",
    "- If $d_{ij}$ is minimum, combine $i$ and $j$ into single object, and go back to the step of computing distances.\n",
    "- Repeat until all objects have been declared jets.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9822dad-6e6e-426a-835b-7640c4fe41a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"https://gsalam.web.cern.ch/panscales/videos/ee-3TeV-bold-50fpd/dijets-001.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dfd895-a835-4f20-b1ce-5799f5b22132",
   "metadata": {},
   "source": [
    "The above animation is taken from [Gavin Salam's webpage](https://gsalam.web.cern.ch/panscales/videos.html) and shows a hadron-level event (end particles are colorless particles made up of quarks called hardons) generated using [pythia 8](https://pythia.org/) (a Monte-Carlo event generator widely used in particle physics). It shows time evolution of an event which is produced from electron-positron collision at center of mass energy of 3 TeV and which initially results in quark-antiquark pair. These quark-antiquark pairs result in a shower of particles as mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ba1d96-675a-4df7-a16e-53bce282c03b",
   "metadata": {},
   "source": [
    "## Import required python libraries\n",
    "- Numpy is used for numerical manipulation, mathematical operations on integers for example. It can be used to create arrays which are python objects that you can think of as matrices (in 1D and 2D case), and perform operations on these arrays. An example of 1-dimensional numpy array is `x = numpy.array([1, 2])`, which you can think of as a row matrix (1 row, 2 columns). An example of 2-dimensional numpy array is `y = numpy.array([1, 2, 3], [3, 4, 5])` which you can think of as a 2x3 matrix (check what `y.shape` returns). \n",
    "- Pandas is a python library for data analysis. It allows one to work with many datasets with formats such as csv, hdf5, etc. \n",
    "- h5py is a library to work with HDF5 files. HDF5 file is a popular format for storing data in high energy physics and astrophysics. \n",
    "- Matplotlib is a plotting python library that can be used to make plots, histograms, heatmaps, etc. \n",
    "- xgboost  is  machine learning library for boosted decision trees (BDTs) which we will use for this hands-on session for creating a model. There are other machine learning libraries like pytorch, tensorflow, etc.\n",
    "- Scikit learn is another machine learning library. It has many useful functions that can be used in conjuction with other machine learning libraries. We will use various scikit learn functions, mainly for preprocessing and visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4099d263-daa7-4558-8008-d1c81a4f8a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b9123a-2823-4c73-b996-0f9c3f32edac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read the train and validation datasets as pandas dataframe\n",
    "The datasets are in HDF5 format. One can use h5py python library to work with HDF5 data format. However, pandas has a convenient function ,`pd.read_hdf()`, to read the HDF5 datasets directly into pandas dataframe. We will use this function to read the train and validation datasets. Note that pandas uses pyTables library under the hood for reading and writing HDF5 files. So, for this to work one also needs to install pyTables (if you are using pip as your python package manager, you can do `pip install --user tables`). For description of how to use `read_hdf`, you can create a new code cell and run `help(pd.read_hdf)` or do a web search. \n",
    "\n",
    "The dataset is slimmed down version of dataset from [HLS4ML LHC jet dataset](https://zenodo.org/records/3601436) and contains 24 high level input features computed for a large radius jet. The large radius jet originates from one of the following particles: gluon (labels == 0), quark (light quarks, labels == 1), and top quark (labels == 2). The jets are clustered using anti-$k_T$ algorithm with $R=0.8$. \n",
    "\n",
    "A brief description of the features is as follows:\n",
    "- Jet mass: Top quark mass distribution has a peak around 172 GeV.  So, mass of top quark sets a jet mass scale of ~170 GeV which is unlike a quark or gluon jet. So mass of the jet can be used as a discriminant. In the dataset, the jet mass is named `j_mass`. \n",
    "- Jet pt: This is the transverse momentum of the jet, $p_T = \\sqrt{p_x^2 + p_y^2}$. Jet $p_T$ variable is named `j_pt`.\n",
    "- Jet multiplicity: It refers to the number of particles making up the jet (jet constituents). It is useful for quark/gluon discrimination.\n",
    "- $\\sum z \\log z$: $z_i = E_i/E_{\\text{parent}}$ is the fraction of energy carried by a jet constituent. $\\sum z \\log z$ is sum over all jet constituents of the product of energy fraction and log of energy fraction. \n",
    "- N-subjettiness ($\\tau_N$): It quantifies how consistent a jet is with having N or more subjets (jets reconstructed with smaller $R$ than the large-R jet, i.e. less than 0.8 in this case). Jets with $\\tau_N \\approx 0$ are deemed to have N or fewer subjets. Jets with $\\tau_N \\gg 0$ are deemed to have at least $N+1$ subjets. (Talk with the instructors if you want more details or unsure what is going on).\n",
    "    <details>\n",
    "    <summary>For those who want details, click on this to get details </summary> \n",
    "    \n",
    "    Essentially, $\\tau_N$ is computed as follows:\n",
    "    - Cluster N candidate subjets inside the large-R jet.\n",
    "    - Loop over the $k$-particles with transverse momentum $p_{T, k}$ that make up the large-R jet (constituent particles), and compute the geometrical distance between the $k^{\\text{th}}$ particle and the $j^{\\text{th}}$ subjet ($j = 1,...,N$).  Find the minimum of the geometrical distances and weight it with the transverse momentum $p_{T,k}$ of the particle. The formula looks like this: \n",
    "    $$\\tau_N = \\frac{1}{d_0} \\sum_k p_{T,k} \\min \\{\\Delta R_{1, k}, \\Delta R_{2,k}, ..., \\Delta R_{N, k}\\}$$\n",
    "    where $\\Delta R_{j,k} = \\sqrt{(\\Delta \\eta)^2 + (\\Delta \\phi)^2}$ is the geometrical distance, and $d_0$ is a normalization factor.\n",
    "    \n",
    "    Note that the initial definition of N-subjettiness was modified to add a power law dependence, so that N-subjettiness is denoted by $\\tau_N^\\beta$, with $\\beta$ denoting the power: \n",
    "    \n",
    "    $$\\tau_N^\\beta = \\frac{1}{d_0} \\sum_k p_{T,k} \\min \\{(\\Delta R)^{\\beta}\\}$$\n",
    "    </details>\n",
    "    \n",
    "     The N-subjettiness variables are named `j_tau*_b*`. For example, `j_tau1_b1` is 1-subjettiness. Note: $\\tau_{\\alpha \\gamma} = \\tau_{\\alpha} / \\tau_{\\gamma}$ . [Click on this link to read the original paper](https://arxiv.org/pdf/1011.2268)\n",
    "- Energy correlation functions: These are quite complicated variables to explain, so we will give some qualitative explanation. \n",
    "    - $M_i^{\\beta}$ variables: These variables identify jets with $i$ particles with high momenta (\"prongs\"; eg a hadronically decaying top quark has final state bqq where b=bottom quark and q = u/d quarks, and thus has \"3-prong\" structure). These are named `j_m*_b*`\n",
    "    - $N_i^{\\beta}$ variables: These variables mimic behavior of subjettiness ratio $\\tau_i/\\tau_{i-1}$. These are named `j_n*_b*`.\n",
    "    - $D_2^{(\\alpha, \\beta)}$: These are variables that can identify boosted two prong jets. These are named `j_d2_a*_b*`.\n",
    "    - $C_i^{\\beta}$ variables: These can also be used to identify i-prong jets. In particular, $C_1^{\\beta}$ can be used to separate quark and gluon when $\\beta$ is small. $C_2^{\\beta}$ can be used for 2-prong jets (for example boosted W boson identification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a75ce75-6bd0-4169-9051-00c7d139361d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_hdf(\"/home/atlas-coffea/train_largeR_jets_qgt.h5\")\n",
    "df_test = pd.read_hdf(\"/home/atlas-coffea/val_largeR_jets_qgt.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f99c8-9c4e-487e-a577-ec0cf4e9fb6b",
   "metadata": {},
   "source": [
    "Now shuffle the dataframe using `sample()` method of pandas DataFrame with `frac` option (try `help(pd.DataFrame.sample())`). Reset the index of the rows of the resulting dataframe using `reset_index` method and drop the old indices (try `help(pd.DataFrame.reset_index)`). Note that you will need to create a validation set by using `frac=0.2` (holding out 20% of train dataset) since xgboost doesn't have feature to automatically split dataset into train and validation set as tensorflow; you will then need to drop the examples you used in validation set from your test set - this can be done as follows: if your train dataframe that you loaded is `df_train`, and after sampling 20% of dataset the validation dataframe is `df_val`, then do `df_train.drop(df_val.index)`; you will also need to reset index of train dataframe. You will use the validation set (the one with name \"val_largeR_jets_qgt.h5\") as test set to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f4798-fa00-4cc6-ab65-f7f82f3581e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(pd.DataFrame.sample)\n",
    "help(pd.DataFrame.reset_index)\n",
    "help(pd.DataFrame.drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0919e-43f8-486d-a5f5-f899d572251a",
   "metadata": {},
   "source": [
    "Now, let's check what the train and validation datasets look like. We will use `head` method of pandas to check a few \"rows\" of data, and `shape` attribute of the dataframes to check their shape. You can also use `describe` method to check what the mean, standard deviation, etc of each column look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25383295-75ba-4883-a059-17e52ba0ce9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(pd.DataFrame.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faf5897-d203-4a5f-ac6a-d6e7e19da16d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(pd.DataFrame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd21b06-73cc-4f15-8f42-fed4f8ed7504",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.DataFrame.describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35486c58-f4e8-4c97-800e-2ab950a57fe0",
   "metadata": {},
   "source": [
    "Now let's take a look at how many training examples we have for each of the unique labels. We will first use boolean mask to select rows corresponding to each unique label which in our case are 0 (gluon), 1 (light quark), and 2 (top). If we have a pandas DataFrame named `df` which has column named `labels`, we can create a boolean mask by using comparison operator `==`. For example, if we want boolean mask for rows whose `labels` column has value 0, we can create a boolean mask as follows: `df['labels'] == 0`. This will return a list containing `True`s and `False`s. This boolean mask can then be used to select all rows of `df` whose `labels` column has value 0: `df[df['labels'] == 0]`. Do this for all unique labels and assign the result to a new dataframe. Then look at the `shape` of those dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023528c-f3c9-478d-9cb9-e31de0e4dab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75ec9639-837f-492e-b222-995ad55248f5",
   "metadata": {},
   "source": [
    "We will remove the column `labels` from dataframe using `pop` method. Do this for train, validation, and test dataframes. So, you will have three label series (variables). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee05d3d9-2440-40c2-9491-e1058647953c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(pd.DataFrame.pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fecd262-cfcf-481a-8cb0-933fe2fc2425",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, let's make plot of the input features using `matplotlib`.  You can get a list of dataframe columns, i.e. `features` using `columns` attribute of the dataframe. We will use `plt.hist` function of matplotlib for making histograms. You can check all the options available for this function by running `help(plt.hist)`. \n",
    "\n",
    "<details>\n",
    "<summary> Click here to see a plotting function in case you are really stuck </summary>\n",
    "    \n",
    "Plotting code\n",
    "```python\n",
    "    def plot_input_feature_histograms(*dfs: list[pd.Series], col_name: str, bins: int, savefig: bool = True,\n",
    "                                  labels: list[str] = None, colors: list[str] = None)->plt.hist:\n",
    "    \"\"\"This function plots histograms for given DataFrame columns that are supplied. The columns should be in a list. \n",
    "    Parameters\n",
    "    -------------------------------------\n",
    "    col_name: str\n",
    "        The name of the column that is to be plotted, used for xlabel.\n",
    "    bins: int\n",
    "        The number of bins to use for plotting.\n",
    "    dfs: list[pd.Series]\n",
    "        A list of pandas Series objects, i.e. objects that hold a column of a pandas DataFrame. For example: [df1['a'], df2['a']].\n",
    "    labels: list[str]\n",
    "        A list of labels to use for plotting. For example ['sample a', 'sample b'].\n",
    "    colors: list[str]\n",
    "        A list of colors to use for plotting. If it is not supplied, default colors will be used. Maximum of 10 default colors are supported.\n",
    "    Returns\n",
    "    ------------------------------------\n",
    "    plt.hist\n",
    "        Returns histograms plotted on same canvas.\n",
    "    \"\"\"\n",
    "    default_colors = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "    if not dfs:\n",
    "        raise ValueError(\"You must supply at least one argument of type pandas.Series.\")\n",
    "    if colors is None:\n",
    "        colors = default_colors[:len(dfs)]\n",
    "    plt.hist(dfs, bins=bins, label = labels, color = colors, histtype='step',  density=True)\n",
    "    plt.xlabel(col_name, fontsize=12)\n",
    "    plt.ylabel(\"Counts\", fontsize=12)\n",
    "    plt.legend()\n",
    "    if savefig:\n",
    "        plt.savefig(f\"features/{col_name}.png\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    return\n",
    "```\n",
    "</details>\n",
    "\n",
    "Make a plot for each of the input features. Which features do you think will help in separating top quark jet from light quark jet or a gluon jet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d1dfe-23e9-4583-9be3-c5a941e8f0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(plt.hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60895be0-2f78-4480-ad06-87d03a648c5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Correlation between features\n",
    "We will use `corr` method of pandas DataFrame to compute pair-wise correlation between the input features. Then we will use seaborn package to create a \"heatmap\" of the correlations. Correlations are usually an indication that two variables have similar information and thus maybe redundant. But note that two highly correlated variables may help a model in different ways. In addition, there might be nonlinear correlations between variables which the `corr` method doesn't support. (For people interested in non-linear correlations, search for **distance correlation**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1129b5d-6ef9-4604-b711-fd7d6ddd3e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(pd.DataFrame.corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80858e65-4a64-4c3a-9d62-03e72ac959db",
   "metadata": {},
   "source": [
    "The dataframe resulting from using `corr` method is a square dataframe with the variable names as row and column names, and the data is the correlation coefficient. Try plotting it using `sns.heatmap`. If you used dataframe with all 24 features, maybe the heatmap is not very legible? You can split it into 4 separate heatmaps using `iloc` attribute of a dataframe to get a new subset DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6b5c3-1a5c-46c9-b1d1-2f808269646c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> If you are stuck with splitting, click here for an example </summary>\n",
    "    \n",
    "```python\n",
    "method = 'pearson'\n",
    "correlation = df_train.corr(method = method)\n",
    "subset_of_correlation = correlation.iloc[0:11, 0:11]\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.heatmap(subset_of_correlation, annot=True, fmt=\".2f\", cmap=\"viridis\", vmin = -1., vmax = 1.0, linewidths = 0.5, cbar_kws = {'label' : method+' correlation'})\n",
    "plt.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6b548-a553-47b6-bc54-6a14db8e3247",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3303173a-6947-437d-8210-dbab25410881",
   "metadata": {},
   "source": [
    "Note that you can create a new dataframe with only a few features and not all features. An example is shown below:\n",
    "```python\n",
    "columns_you_want = [\n",
    "    'j_c1_b1', 'j_c1_b2', 'j_c2_b1', 'j_c2_b2', 'j_d2_a1_b1_mmdt', 'j_d2_a1_b2_mmdt', \n",
    "    'j_d2_b1_mmdt', 'j_d2_b2_mmdt', 'j_m2_b1_mmdt', 'j_m2_b2', 'j_mass', \n",
    "    'j_multiplicity'\n",
    "]\n",
    "final_df_train = df_train[columns_you_want]\n",
    "```\n",
    "\n",
    "Now, based on the correlations, do you want to use all the features or select a subset? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d47c2b-b5d9-43a4-8033-42613d627c72",
   "metadata": {},
   "source": [
    "It's time to build a model. You can use `xgb.XGBClassifier` for training. There are many parameters that you can adjust. Some of them are as follows ([checkout this document for all the parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)):\n",
    "- `tree_method`: This lets you decide which algorithm to use for training the BDT. There are three builtin tree methods: `exact`, `approx`, and `hist`. The exact method is more accurate, but it is slow. Try with one of these methods.\n",
    "- `max_depth`: A deeper depth means the model will be more complex (and can lead to overfitting)\n",
    "- `n_estimators`: number of boosted decision trees.\n",
    "- `eval_metric`: It is a metric to monitor model's performance. In this case you can try to use metrics like `auc` (area under the curve), `mlogloss` (multi-class log loss, read below), etc. \n",
    "- `early_stopping_rounds`: If you want, you can stop the training early if the loss does not improve after some iterations using this parameter. Set it to some number like 5. \n",
    "- `objective`: The function to use for measuring how well the model describes the training data. It is a combination of a loss function and regularization term (to help with avoiding overfitting). For classification, you can use `multi:softprob` as the objective. This is essentially using cross-entropy as loss function (output of softmax is used to compute loss function). Read below on explanations. \n",
    "\n",
    "Note: An example of loss function for regression is mean squared error (MSE): $\\sum (y_{\\text{actual}})^2 - y_{\\text{pred}}$. Suppose you have a model that predicts the price of a house based on various features such as the location, amenities, etc. If your model predicts $100$k for a house when the actual price is $1$ million, then your model hasn't quite learned the actual model underlying the data; in this case MSE would be large, thus telling that the model needs to do better. \n",
    "\n",
    "The goal of machine learning is essentially to minimze the loss - the model needs to change its weights so that loss is reduced, i.e. move in the direction opposite to the increasing loss. This is done by using gradient of loss function with respect to the weights. Since direction of gradient is the direction of steepest ascent, if we move in the direction opposite to the gradient of loss function, we will minimize the loss. This can be seen from a simple 2 dimensional plot of $x^2$. The global minimum is at $x = 0$ and if you move in either $x>0$ direction or $x < 0$ direction, value of $x^2$ increases. The derivative of $x^2$ is $2x$. For $x < 0$, the derivative is going to be negative; if you move in the opposite direction to the derivative, that is in the direction of positive x, you will eventually reach the minima, $x = 0$. For $x > 0$, the derivative is going to be positive, so if you move in the direction opposite to positive $x$, i.e. in the direction of negative $x$, you will eventually reach the minima, $x = 0$.\n",
    "<div style=\"display: flex\">\n",
    "<div style=\"flex: 1; padding: 10px;\">\n",
    "    <center>\n",
    "        <img src=\"parabola.png\" alt=\"plot of x squared\" width=\"300\" height=\"200\">\n",
    "    </center>\n",
    "    <p>\n",
    "        This image shows a plot of parabola with center at origin, i.e. plot of $x^2$\n",
    "    </p>\n",
    "</div>\n",
    "<div style=\"flex: 1; padding: 10px;\">\n",
    "    <center>\n",
    "        <img src=\"derivative_x_squared.png\" alt=\"plot of 2x\" width=\"300\" height=\"300\">\n",
    "    </center>\n",
    "    <p>\n",
    "        This image shows derivative of $x^2$, i.e. plot of $2x$\n",
    "    </p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "For this classification task, we will use multi-classifier version of cross-entropy loss function (mlogloss). For binary classification, cross-entropy of an example/event is given by $-[y \\log(p) + (1-y)\\ \\log(1-p)]$, where $y$ is the actual label, and $p$ is the output score of softmax (can also be written as $- \\sum_{i = 1}^2 y_i \\cdot log(p_i)$ with $p_1 = p$ and $p_2 = 1-p_1 = 1-p$). If the correct label is $y=1$ and $p$ is close to one, this quantity would be close to 0. If the correct label is $y = 0$ and $p$ is close to 0, then again this quantity would be close to 0. If values of $y$ and $p$ are not close, then the quantity would be further away from 0. One can take an average of cross-entropy across all data samples to get total cross-entropy loss. This can be extended to multi-classification by simply summing the loss over the classes: $-\\frac{1}{N}\\Sigma_{i=1}^N\\Sigma_{j=1}^C(y_{i,j} \\cdot log(p_{i,j}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cafaab7-5849-4408-a46d-20c62d317197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(xgb.XGBClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0cd8a-7fcd-4f80-af7b-011e162c1ca6",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can use `predict` method of the model to get predicted class for an example/event. Apply this method on the test dataset and use the result with `classification_report` from scikit learn to give you a report on your evaluation metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05afbad2-fbc6-4285-95e0-9e11d3cf0b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(xgb.XGBClassifier.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae2297-8d65-4ff3-ba51-2445d1a3fe8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f6032-aac2-459f-8b63-993fbe6c73f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will use scikit-learn's permutation importance to check what importance each feature is assigned for a particular model. Higher value of importance means the model is learning more from that particular feature (note that the importances are model dependent, so make sure your model is good before computing the feature importance). One complication with checking permutation importance is the fact that it does not work with output of softmax... So we need to convert the predicted scores to predicted label. This can be done using `np.argmax` method with `axis = 1` (columns instead of rows). \n",
    "<details>\n",
    "    <summary> It's a bit complicated, so click here to get a `scoring` function and use it as argument `scoring` for `permutation_importance` method </summary>\n",
    "    \n",
    "```python\n",
    "    def scoring(model: xgb.XGBClassifier, x: pd.DataFrame , y: pd.Series):\n",
    "    \"\"\"Returns a scoring callable so that Sklearn's permutation importance works with output from predict_proba method of XGBClassifier.\n",
    "    Parameters\n",
    "    -------------\n",
    "    model: xgb.XGBClassifier\n",
    "        A xgboost model for which the permutation importance is to be computed.\n",
    "    x: pd.DataFrame\n",
    "        A pandas dataframe that is to be used for permutation importance.\n",
    "    y: pd.Series\n",
    "        A pandas series for labels corresponding to the dataset x. \n",
    "    Returns\n",
    "    -------------\n",
    "    Accuracy score computed from predicted labels and actual labels. \n",
    "    \"\"\"\n",
    "    y_pred_probability = model.predict_proba(x)\n",
    "    # Get the index of the element with highest probability\n",
    "    y_pred = np.argmax(y_pred_probability, axis=1) \n",
    "    return accuracy_score(y, y_pred)\n",
    "```    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f61ba-4505-4adf-980f-cff81a144b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = permutation_importance(\"fill in required arguments here\", scoring=scoring)\n",
    "bdt_importances = pd.Series(result.importances_mean, index=final_df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8600bc3-ebdb-47ba-8f73-64b97ee291fc",
   "metadata": {},
   "source": [
    "Make predictions using the test features  using `predict_proba` method of your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747308ad-27c9-4539-aaa1-cf92aef0494a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(xgb.XGBClassifier.predict_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bf67d5-11ec-415a-863b-c19e7db96cd8",
   "metadata": {},
   "source": [
    "Now we will plot confusion matrix. A confusion matrix tells us how many examples our model gets right. A good model will get many examples right compared to incorrect predictions. The confusion matrix can be plotted using `confusion_matrix` method from scikit-learn. You can use the predicted scores from just before but you need to use `np.argmax` to convert the output of softmax to a label (as you did while computing permutation importance). Use seaborn to plot the confusion matrix after computing it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508933b-489f-4b9b-a6bf-847d4ee0142f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b64e5a3-1776-45a0-a817-f13c1764f249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_labels, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff21245-b6d9-4bbe-ba58-45bbd3118f6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we will make some plots to see how well your model separates \"signal\" (top jet) from the \"background\" (gluon/light quarks jet). In your test set, you have information on which \"event\" comes from which jet (the labels). You can use the labels to directly plot one of the softmax scores (output of `predict_proba` method) corresponding to your class of interest. For example, if your class of interest is top (label = 2), you would use the third score in the softmax output (remember that softmax outputs three probabilities corresponding to the three classes). However, we will use a logratio and plot it. For each label/class, compute $\\log ((p_i)/(p_j + p_k))$; for example, if your interest is top jet, you would compute $\\log((p_2)/(p_0 + p_1))$ where $p_2$ is the softmax probability corresponding to top class (third score), $p_0$ is the softmax probability corresponding to gluon jet class, and $p_1$ is the softmax probability corresponding to quark jet class. Then plot this log ratio for the three classes in the same plot. (if this doesn't make sense, talk with the instructor).\n",
    "\n",
    "<details>\n",
    "    <summary> In case you are stuck, click to  go through a function that gives you list of log ratio scores per class </summary>\n",
    "    \n",
    "```python\n",
    "    def getListOfScoresPerClass(actual_labels: np.array, prediction: np.array, index_numerator: int):\n",
    "        \"\"\"Returns a list of log ratio scores corresponding to each class. \n",
    "        Parameters\n",
    "        ----------------\n",
    "        actual_labels: np.array\n",
    "            Numpy array or pandas series for the actual labels.\n",
    "        prediction: np.array\n",
    "            Predictions obtained from the model, i.e. the output of `predict_proba` method. \n",
    "        index_numerator: int\n",
    "            The label whose score is to be used in the numerator while computing the log ratio of probabilities. \n",
    "            \n",
    "        Returns\n",
    "        ----------------\n",
    "        dfs\n",
    "            A list of lists with log ratio scores. The 0th element of the list is a list containing log ratio scores corresponding to label 0, 1st element is a list containing log ratio scores corresponding to label 1, and so on. \n",
    "        \"\"\"\n",
    "        labels = [0, 1, 2]\n",
    "        labels.remove(index_numerator)\n",
    "        logratio = [np.log(x[index_numerator]/(x[labels[0]] + x[labels[1]])) for x in prediction]\n",
    "        dfs = []\n",
    "        dfs.append([val for val, mask in zip(logratio, actual_labels == 0) if mask])\n",
    "        dfs.append([val for val, mask in zip(logratio, actual_labels == 1) if mask])\n",
    "        dfs.append([val for val, mask in zip(logratio, actual_labels == 2) if mask])  \n",
    "    return dfs\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89573cd3-08c6-46f2-8f14-2d9c1ad0995c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34802ac9-5bed-4e51-8102-b4c6b52eb76e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, apply a \"cut\" on the log ratio. Basically, require that the log ratio is greater than or smaller than some number so that you have higher number of top quark jets passing the cut and low number of other jets. After applying the cut, check how many top jets, gluon jets, light quark jets pass the cut. You can also check the percentage of top jets that pass the cut, and so on for other jets; for instance, if your validation sample had `n_t` total top jets, and `m_t` top jets pass the cut, get the ratio `m_t/n_t`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8852a450-4409-46f4-b201-a5693eae7d18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
